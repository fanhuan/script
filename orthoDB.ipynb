{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download all protein sequences in orthoDB v11 for one/multiple species\n",
    "\n",
    "1. odb11v0_OG2genes.tab.gz were downloaded from https://data.orthodb.org/download/\n",
    "wget https://data.orthodb.org/download/odb11v0_OG2genes.tab.gz\n",
    "2. get the OG ids that are related to oil palm (taxid 51953)\n",
    "zcat odb11v0_OG2genes.tab.gz | grep 51953_0 | cut -f 1 | sort | uniq > OG_51953_0.tab\n",
    "3. Download the oil palm protein sequences from the clusters that contains oil palm sequences\n",
    "nohup sh download.sh > logfile 2>&1 < /dev/null &\n",
    "\n",
    "download.sh:\n",
    "#!/bin/bash\n",
    "output_file=\"combined_data.tsv\"\n",
    "\n",
    "while IFS= read -r line; do\n",
    "    curl \"https://data.orthodb.org/current/fasta?id=$line&species=51953\" -L -k >> \"$output_file\"\n",
    "done < \"OG_51953_0.tab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl 'https://data.orthodb.org/current/blast?seq=MGQMGGPDGDGPHHQYHYQALLAAVQNPSQGLHVPLHAGAGAPAAGPGPRPGADADASSTHNANATPHSQPPRAFTDWSASNSAFAAQPAPATTNTPFHYNLSQSYALWTHYMLNKNVSYSTYSTPHEPLRHTHIPDKYSGCAFSLGFDSFTTMSLGPNICANMTPMERSISAKEPENSEDLPTVVRSSDEMDTRNSGDVRRDTVDTLPESKQSHESCASVSNKFDSGEYQVILRKELTKSDVANSGRIVLPKKDAEAGLPPLVQGDPLILQMDDMVLPIIWKFKYRFWPNNKSRMYILEAAGEFVKTHGPSGRGYAHYLQKLRTWQIYYPWGEVHSADNP' -L -o blast.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl 'https://data.orthodb.org/current/blast?seq=MGQMGGPDGDGPHHQYHYQALLAAVQNPSQGLHVPLHAGAGAPAAGPGPRPGADADASSTHNANATPHSQPPRAFTDWSASNSAFAAQPAPATTNTPFHYNLSQSYALWTHYMLNKNVSYSTYSTPHEPLRHTHIPDKYSGCAFSLGFDSFTTMSLGPNICANMTPMERSISAKEPENSEDLPTVVRSSDEMDTRNSGDVRRDTVDTLPESKQSHESCASVSNKFDSGEYQVILRKELTKSDVANSGRIVLPKKDAEAGLPPLVQGDPLILQMDDMVLPIIWKFKYRFWPNNKSRMYILEAAGEFVKTHGPSGRGYAHYLQKLRTWQIYYPWGEVHSADNP' -L -o blast.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse the search.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"/bioinfo/palm/ref/orthoDBv11/search.dat\"\n",
    "\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    data_dict = json.load(json_file)\n",
    "\n",
    "ortholog_path = \"/bioinfo/palm/ref/orthoDBv11/orthologs.dat\"\n",
    "\n",
    "with open(ortholog_path, \"r\") as json_file:\n",
    "    ortholog_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parse the downloaded protein sequence to remove the redundent ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import json\n",
    "\n",
    "sequence_dic = {}\n",
    "tag_dic = {}\n",
    "level_dic = {}\n",
    "with open('/bioinfo/palm/ref/orthoDBv11/oil_palm_protein.fa','w') as fh:\n",
    "    for record in SeqIO.parse('/bioinfo/palm/ref/orthoDBv11/combined_data.tsv','fasta'):\n",
    "        if record.id in sequence_dic:\n",
    "            # gene already wrote out, check whether the sequence is different\n",
    "            if record.seq != sequence_dic[record.id]:\n",
    "                # need to write to fh if the sequence is different\n",
    "                gene, description = record.description.split(' ', 1)\n",
    "                description = description.strip()\n",
    "                description_dict = json.loads(description)\n",
    "                cluster = description_dict['pub_og_id']\n",
    "                record.id = record.id + '_' + cluster\n",
    "                tag_dic[gene] = cluster\n",
    "                SeqIO.write(record, fh, 'fasta')\n",
    "                print(gene) #nothing was printed, which means the protein sequences always stay the same no matter the node\n",
    "        else:\n",
    "            SeqIO.write(record, fh, 'fasta')\n",
    "        sequence_dic[record.id] = record.seq\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now a simpler code with the understanding that the protein sequences always stay the same for the same gene no matter the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import json\n",
    "\n",
    "sequence_dic = {}\n",
    "level_dic = {}\n",
    "with open('/bioinfo/palm/ref/orthoDBv11/oil_palm_protein.fa','w') as fh:\n",
    "    for record in SeqIO.parse('/bioinfo/palm/ref/orthoDBv11/combined_data.tsv','fasta'):\n",
    "        gene, description = record.description.split(' ', 1)\n",
    "        description = description.strip()\n",
    "        description_dict = json.loads(description)\n",
    "        cluster = description_dict['pub_og_id']\n",
    "        level = cluster.split('at')[1]\n",
    "        if level in level_dic:\n",
    "            level_dic[level].append(gene)\n",
    "        else:\n",
    "            level_dic[level] = [gene]\n",
    "        if record.id not in sequence_dic:\n",
    "            SeqIO.write(record, fh, 'fasta')\n",
    "            sequence_dic[record.id] = record.seq\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_dict = {}\n",
    "\n",
    "# Iterate through the dictionary values and calculate lengths\n",
    "for key, value in level_dic.items():\n",
    "    unique_items = set(value)  # Create a set to store unique items\n",
    "    lengths_dict[key] = len(unique_items)\n",
    "\n",
    "print(lengths_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means one gene is classified into only one cluster at most at one level. \n",
    "\n",
    "hen we used orthomapper to map all dura proteins to orthodb v11. Results are in /bioinfo/tools/orthologer/Results\n",
    "\n",
    "Now let's get the protein sequences that wasn't placed to any orthodb clusters (14632).\n",
    "\n",
    "Note that if a gene was not placed in orthoDBv11 by othomapper, it won't be found in the reciprocal mapping between dura and EG5 inOrthoDBv11 either. \n",
    "\n",
    "Now what do we do? Map to EG5 just to see what they could have done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "\n",
    "file_path = 'dura.og.annotations'\n",
    "df = pd.read_csv(file_path, sep='\\t', header=0)\n",
    "df.rename(columns={'#query': 'query'}, inplace=True)\n",
    "df_eg5 = pd.read_csv('dura_EG5_orthoDB11.tbl', sep='\\t', header=None)\n",
    "i = 0\n",
    "with open('dura_proteins_not_in_orthodbv11.fasta','w') as fh:\n",
    "    for record in SeqIO.parse('../dura/dura_proteins.fasta','fasta'):\n",
    "        if record.id not in df['query'].values:\n",
    "            if record.id in df_eg5[0]:\n",
    "                print(record.id)\n",
    "            SeqIO.write(record, fh, 'fasta')\n",
    "            i += 1\n",
    "print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix the ID in EG5.1 cds and protein.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "in_file = '/home/huan/build/orthologer/EG5.1_Genes.V3.na.fa'\n",
    "out_file = '/home/huan/build/orthologer/EG5.1_Genes.V3.newid.na.fa'\n",
    "with open(out_file,'w') as fh:\n",
    "    for record in SeqIO.parse(in_file,'fasta'):\n",
    "        record.id = record.description.split()[1].split('=')[1]\n",
    "        SeqIO.write(record, fh, 'fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the orthomapper results from all three levels, with priority to lower levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "\n",
    "name = 'maize'\n",
    "\n",
    "file1 = '~/build/orthologer/Results/{}_4447.og.annotations'.format(name)\n",
    "df1 = pd.read_csv(file1, sep='\\t', header=0)\n",
    "df1.rename(columns={'#query': 'query'}, inplace=True)\n",
    "\n",
    "file2 = '~/build/orthologer/Results/{}_3193.og.annotations'.format(name)\n",
    "df2 = pd.read_csv(file2, sep='\\t', header=0)\n",
    "df2.rename(columns={'#query': 'query'}, inplace=True)\n",
    "\n",
    "df2_new = df2[~df2['query'].isin(df1['query'])]\n",
    "df12 = pd.concat([df1, df2_new], ignore_index=True)\n",
    "\n",
    "file3 = '~/build/orthologer/Results/{}_33090.og.annotations'.format(name)\n",
    "df3 = pd.read_csv(file3, sep='\\t', header=0)\n",
    "df3.rename(columns={'#query': 'query'}, inplace=True)\n",
    "\n",
    "df3_new = df3[~df3['query'].isin(df12['query'])]\n",
    "df123 = pd.concat([df12, df3_new], ignore_index=True)\n",
    "\n",
    "# change their names to gene names (this only applies to EG5.1)\n",
    "if name == 'EG5.1':\n",
    "    header = pd.read_csv('~/build/orthologer/EG5.1_aa.header', sep=' ', header=None, names=['old','new','len'])\n",
    "    header['old'] = header['old'].str[1:]\n",
    "    header['new'] = header['new'].str.split('=').str[1]\n",
    "    header.drop('len', axis=1, inplace=True)\n",
    "    dictionary = {key: value for key, value in zip(header['old'], header['new'])}\n",
    "    df123['query'] = df123['query'].map(dictionary)\n",
    "\n",
    "df123.to_csv('~/build/orthologer/{}_orthoDBv11_4447_3193_33090.og.annotations'.format(name), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dura gene-> og -> at gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/home/huan/build/orthologer/EG5.1_orthoDBv11_4447_3193_33090.og.annotations', sep='\\t', header=0)\n",
    "df1.rename(columns={'#query': 'query'}, inplace=True)\n",
    "\n",
    "df2 = pd.read_csv('/home/huan/build/orthologer/maize_orthoDBv11_4447_3193_33090.og.annotations', sep='\\t', header=0)\n",
    "df2.rename(columns={'#query': 'query'}, inplace=True)\n",
    "\n",
    "df3 = pd.read_csv('/home/huan/build/orthologer/rice_orthoDBv11_4447_3193_33090.og.annotations', sep='\\t', header=0)\n",
    "df3.rename(columns={'#query': 'query'}, inplace=True)\n",
    "\n",
    "df4 = pd.read_csv('/home/huan/build/orthologer/Arabidopsis_orthoDBv11_4447_3193_33090.og.annotations', sep='\\t', header=0)\n",
    "df4.rename(columns={'#query': 'query'}, inplace=True)\n",
    "\n",
    "df5 = pd.read_csv('/home/huan/build/orthologer/dura_orthoDBv11_4447_3193_33090.og.annotations', sep='\\t', header=0)\n",
    "df5.rename(columns={'#query': 'query'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_csv('/home/huan/build/orthologer/dura_orthoDBv11_4447_3193_33090.og.annotations', sep='\\t', header=0)\n",
    "df5.rename(columns={'#query': 'query'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gff = pd.read_csv('/home/huan/build/orthologer/Araport11_GFF3_genes_transposons.symbol.gff.2', sep='\\t', header=None, comment=\"#\", names=['chr','source','type','start','end','score','strand','phase','attributes'])\n",
    "gff_mRNA = gff[gff['type']=='mRNA']\n",
    "\n",
    "# Apply the function to each row in the 'attribute' column\n",
    "parsed_attributes = gff_mRNA['attributes'].apply(parse_attribute).apply(pd.Series)\n",
    "\n",
    "# Join the parsed attributes back to the original DataFrame\n",
    "gff_mRNA = gff_mRNA.join(parsed_attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gff_light = gff_mRNA[['symbol','ID']]\n",
    "gff_light.rename(columns={'ID': 'query'}, inplace=True)\n",
    "df4_merged = pd.merge(df4, gff_light, on='query',how='inner')\n",
    "df4_light = df4_merged[['ODB_OG','symbol']].drop_duplicates()\n",
    "df4_light = df4_light[df4_light['symbol'] != 'None']\n",
    "df4_light = df4_light.groupby('ODB_OG')['symbol'].agg(lambda x: ';'.join(x.astype(str))).reset_index()\n",
    "df1_merged = pd.merge(df1, df4_light, on='ODB_OG',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_merged = pd.merge(df5, df4_light, on='ODB_OG',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_light.to_csv('/home/huan/build/orthologer/Arabidopsis_OG_symple.map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_merged.to_csv('/home/huan/build/orthologer/EG5.1_orthoDBv11_4447_3193_33090.og.symbol.annotations', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_merged.to_csv('/home/huan/build/orthologer/dura_orthoDBv11_4447_3193_33090.og.symbol.annotations', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gff = pd.read_csv('/home/huan/build/orthologer/EG5.1_Genes.V3.newid.gff3', sep='\\t', header=None, comment=\"#\", names=['chr','source','type','start','end','score','strand','phase','attributes'])\n",
    "gff_gene = gff[gff['type']=='gene']\n",
    "parsed_attributes = gff_gene['attributes'].apply(parse_attribute).apply(pd.Series)\n",
    "\n",
    "# Join the parsed attributes back to the original DataFrame\n",
    "gff_gene = gff_gene.join(parsed_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge snpEff files from different chromosomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# Define the pattern to match your TSV files\n",
    "file_pattern = \"/home/huan/build/orthologer/EG5.1_batch2_190_chr*.snpEff.genes.txt\"\n",
    "\n",
    "# Use glob to get a list of file paths\n",
    "file_paths = glob.glob(file_pattern)\n",
    "\n",
    "# List to hold each DataFrame\n",
    "dfs = []\n",
    "\n",
    "# Loop through the file paths, read each file into a DataFrame, and add it to the list\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path, sep='\\t', header = 1)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "combined_df.rename(columns={'#GeneName': 'GeneName'}, inplace=True)\n",
    "\n",
    "# Check the combined DataFrame\n",
    "combined_df.to_csv('/home/huan/build/orthologer/EG5.1_batch2_190.snpEff.genes.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plant_hormone_gene_list = ['TIR1','Aux','IAA','ARF','YUCCA','PIN','GAI','SPY','GA20ox','GA3ox','GA2ox','ARR','CRE1','WOL','AHK4','ETR1','CTR1','EIN2','PYR','PYL','RCAR','ABI','BRI1','BIN2','BZR1','COI1','JAZ','LOX', 'AOS', 'AOC','MAX1','D14','D53','SMXLs','NPR1']\n",
    "MADS_gene_list = ['AG','AP1','AP3','PI','SEP1','SOC1','FLC','SVP','SHP2','SEP3']\n",
    "homeobox_gene_list = ['WUS','KNAT','GL2','WOX','HB-','BEL1','PHB','ATH1']\n",
    "phyto_list = ['PHYA','PHYB','PHYC','PHYD','PHYE','PIF','PIL']\n",
    "#PIF\n",
    "gene_list = phyto_list.copy()\n",
    "outbed = '/home/huan/build/orthologer/EG5.1_PIF.bed'\n",
    "outfile = '/home/huan/build/orthologer/PIF.gene.list'\n",
    "# Plant hormone\n",
    "#gene_list = plant_hormone_gene_list.copy()\n",
    "#outbed = '/home/huan/build/orthologer/EG5.1_plant_hormone.bed'\n",
    "#outfile = '/home/huan/build/orthologer/plant_hormone.gene.list'\n",
    "# MADS-box\n",
    "#gene_list = MADS_gene_list.copy()\n",
    "#outbed = '/home/huan/build/orthologer/EG5.1_MADS_box.bed'\n",
    "#outfile = '/home/huan/build/orthologer/MADS_box.gene.list'\n",
    "# homeobox\n",
    "#gene_list = homeobox_gene_list.copy()\n",
    "#outbed = '/home/huan/build/orthologer/EG5.1_homeobox.bed'\n",
    "#outfile = '/home/huan/build/orthologer/Homeobox.gene.list'\n",
    "\n",
    "snpEff = pd.read_csv('/home/huan/build/orthologer/EG5.1_batch2_190.snpEff.genes.csv')\n",
    "snpEff['ID'] =  snpEff['TranscriptId'].str[:-2]\n",
    "snpEff.columns = [col.lstrip('variants_') if col.startswith('variants_') else col for col in snpEff.columns]\n",
    "out_snp = outbed + '.snpEff.csv'\n",
    "with open(outbed,'w') as fh_bed:\n",
    "    with open(outfile, 'w') as fh_file:\n",
    "        fh_file.write(','.join(['Gene', 'OG_ID', 'EG5.1', 'Dura', 'Arabidopsis', 'Rice', 'Maize']) + '\\n')\n",
    "        for gene in gene_list:\n",
    "            ogs = df4_light[df4_light['symbol'] == gene]['ODB_OG']\n",
    "            if ogs.shape[0] == 0:\n",
    "                pattern = rf'{gene}[1-9][0-9]*'\n",
    "                ogs = df4_light[df4_light['symbol'].str.match(pattern)]['ODB_OG']    \n",
    "            for og in ogs:\n",
    "                genes = df4_light[df4_light['ODB_OG'] == og]['symbol']\n",
    "                for gene in genes:\n",
    "                    EG_genes = df1[df1['ODB_OG'] == og]['query']\n",
    "                    gff_sub = gff_gene[gff_gene['ID'].isin(EG_genes)]\n",
    "                    EG_genes = ';'.join(EG_genes)\n",
    "                    EG_genes = ';'.join(df1[df1['ODB_OG'] == og]['query'])\n",
    "                    maize_genes = ';'.join(df2[df2['ODB_OG'] == og]['query'])\n",
    "                    rice_genes = ';'.join(df3[df3['ODB_OG'] == og]['query'])\n",
    "                    at_genes = ';'.join(df4[df4['ODB_OG'] == og]['query'])\n",
    "                    dura_genes = ';'.join(df5[df5['ODB_OG'] == og]['query'])\n",
    "                    fh_file.write(','.join([gene, og, EG_genes, dura_genes, at_genes, rice_genes, maize_genes]) + '\\n')\n",
    "                    for index, row in gff_sub.iterrows():\n",
    "                        fh_bed.write('\\t'.join([row['chr'],str(row['start']),str(row['end']),row['ID'], gene, og]) + '\\n')\n",
    "\n",
    "bed = pd.read_csv(outbed, sep = '\\t', header = None, names= ['chr','start','end','ID','gene','og'])\n",
    "bed_merge = pd.merge(bed, snpEff, on='ID',how='inner').drop(['GeneName','GeneId'], axis=1)\n",
    "bed_merge.to_csv(out_snp, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MADS-box genes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno = pd.read_csv('/home/huan/build/orthologer/EG5.1_orthoDBv11_4447_3193_33090.og.annotations', sep = '\\t')\n",
    "filtered_df = anno[anno['Description'].str.contains('MADS-box')]\n",
    "IDs = [i.split(',') for i in filtered_df['Interpro']]\n",
    "EG5_ID = set([item for sublist in IDs for item in sublist])\n",
    "anno = pd.read_csv('/home/huan/build/orthologer/dura_orthoDBv11_4447_3193_33090.og.annotations', sep = '\\t')\n",
    "filtered_df = anno[anno['Description'].str.contains('MADS-box')]\n",
    "IDs = [i.split(',') for i in filtered_df['Interpro']]\n",
    "dura_ID = set([item for sublist in IDs for item in sublist])\n",
    "anno = pd.read_csv('/home/huan/build/orthologer/rice_orthoDBv11_4447_3193_33090.og.annotations', sep = '\\t')\n",
    "filtered_df = anno[anno['Description'].str.contains('MADS-box')]\n",
    "IDs = [i.split(',') for i in filtered_df['Interpro']]\n",
    "rice_ID = set([item for sublist in IDs for item in sublist])\n",
    "anno = pd.read_csv('/home/huan/build/orthologer/maize_orthoDBv11_4447_3193_33090.og.annotations', sep = '\\t')\n",
    "filtered_df = anno[anno['Description'].str.contains('MADS-box')]\n",
    "IDs = [i.split(',') for i in filtered_df['Interpro']]\n",
    "maize_ID = set([item for sublist in IDs for item in sublist])\n",
    "anno = pd.read_csv('/home/huan/build/orthologer/Arabidopsis_orthoDBv11_4447_3193_33090.og.annotations', sep = '\\t')\n",
    "filtered_df = anno[anno['Description'].str.contains('MADS-box')]\n",
    "IDs = [i.split(',') for i in filtered_df['Interpro']]\n",
    "AT_ID = set([item for sublist in IDs for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union = list(set(EG5_ID) | set(dura_ID) | set(rice_ID) | set(maize_ID) | set(AT_ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MADS-box interpro IDs are: ['IPR033897','IPR002487','IPR002100','IPR031050','IPR036879','IPR033896']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a='MGRGRVQLKRIENKINRQVTFSKRRAGLLKKAHEISVLCDAEVALVVFSHKGKLFEYSTDSCMEKILERYERYSYAERQLIAPESDVNTNWSMEYNRLKAKIELLERNQRHYLGEDLQAMSPKELQNLEQQLDTALKHIRTRKNQLMYESINELQKKEKAIQEQNSMLSKQIKEREKILRAQQEQWDQQNQGHNMPPPLPPQQHQIQHPYMLSHQPSPFLNMGGLYQEDDPMAMRRNDLELTLEPVYNCNLGCFAA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "with open('/home/huan/build/orthologer/IPR002100.gene') as fh:\n",
    "    for line in fh:\n",
    "        dic[line.rstrip()] = ''\n",
    "with open('/home/huan/build/orthologer/dura_IPR002100_proteins.fasta','w') as outfh:\n",
    "    for record in SeqIO.parse('/home/huan/build/orthologer/dura_proteins.fasta','fasta'):\n",
    "        if record.id in dic:\n",
    "            SeqIO.write(record, outfh, 'fasta')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a1, a2, a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len('SDVNTNWSMEYNRLKAKIELLERNQRHYLGEDLQAMSPKELQNLEQQLDTALKHIRTRKNQLMYESINELQKKEKAIQEQNSMLSKQI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:172]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lists = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "flattened_list = [item for sublist in list_of_lists for item.spli in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, now I need to write a script where it takes a symbol, and returns the gene name i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "df1 = pd.DataFrame({\n",
    "    'attribute': ['ID=AT1G01010.1;Parent=AT1G01010;Name=AT1G01010.1']\n",
    "})\n",
    "\n",
    "\n",
    "# Apply the function to each row in the 'attribute' column\n",
    "parsed_attributes = df1['attribute'].apply(parse_attribute).apply(pd.Series)\n",
    "\n",
    "# Join the parsed attributes back to the original DataFrame\n",
    "df1 = df1.join(parsed_attributes)\n",
    "\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union = set(df2['ODB_OG']) | set(df3['ODB_OG']) | set(df4['ODB_OG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_og_all = set(df1['ODB_OG']) & union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shared_og_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_og = set(df1['ODB_OG']) & set(df2['ODB_OG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shared_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shared_og)/23056"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df1['ODB_OG'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12593/15674"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we need to go from gene name to clusterID, then dura gene ID.\n",
    "Play with the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, json\n",
    "\n",
    "gene_name = 'ABI5'\n",
    "level = 4447\n",
    "cmd = \"curl 'https://data.orthodb.org/current/search?query={}&level={}' -L -o {}_at{}.dat\".format(gene_name, level, gene_name, level)\n",
    "print(cmd)\n",
    "subprocess.run(cmd, shell=True, check=True)\n",
    "with open(\"{}_at{}.dat\".format(gene_name, level), \"r\") as json_file:\n",
    "    data_dict = json.load(json_file)\n",
    "anno_file = 'dura_orthoDBv11_4447_3193_33090.og.annotations'\n",
    "df = pd.read_csv(anno_file, sep='\\t', header=0)\n",
    "selected_rows = df[df['ODB_OG'].isin(data_dict['data'])]\n",
    "selected_rows.rename(columns={'#query': 'query'}, inplace=True)\n",
    "i = 0\n",
    "protein_file = '/bioinfo/palm/ref/dura/dura_proteins.fasta'\n",
    "with open('dura_proteins_{}_at{}.aa'.format(gene_name, level),'w') as fh:\n",
    "    for record in SeqIO.parse(protein_file,'fasta'):\n",
    "        if record.id in selected_rows['query'].values:\n",
    "            cluster = selected_rows.loc[selected_rows['query'] == record.id, 'ODB_OG']\n",
    "            tag = selected_rows.loc[selected_rows['query'] == record.id, 'Description']\n",
    "            if len(tag) == 1:\n",
    "                record.description = ':'.join([cluster.iloc[0], tag.iloc[0]])\n",
    "            else:\n",
    "                print(record.id + 'appeared in more than one cluster?')\n",
    "\n",
    "            SeqIO.write(record, fh, 'fasta')\n",
    "            i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selected_rows.loc[selected_rows['query'] == 'Egu023084-mRNA-1', 'ODB_OG'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We ran orthoDB on four other species and compared them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "level = 4447\n",
    "def og_dic(level, name):\n",
    "    # this function takes a orthoDB annotation, count the cluser compotion and return a pd\n",
    "    anno_file = \"/bioinfo2/palm/ref/orthoDBv11/{}_{}.og.annotations\".format(name, level)\n",
    "    df = pd.read_csv(anno_file, sep='\\t', header=0)\n",
    "    dic = Counter(df['ODB_OG'])\n",
    "    # Convert dictionaries to DataFrames\n",
    "    df = pd.DataFrame(list(dic.items()), columns=['key', name]).set_index('key')\n",
    "    return(df)\n",
    "\n",
    "df_dura = og_dic(4447, 'dura')\n",
    "df_coco = og_dic(4447, 'coco')\n",
    "df_date = og_dic(4447, 'date')\n",
    "df_picifera = og_dic(4447, 'EG5')\n",
    "df_rice = og_dic(4447, 'rice')\n",
    "\n",
    "df = pd.concat([df_dura, df_coco, df_date, df_picifera, df_rice], axis=1)\n",
    "\n",
    "df.to_csv(\"/bioinfo2/palm/ref/orthoDBv11/combined_og.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df123[df123.duplicated('query')]  # Replace 'column_name' with the actual column name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "anno_file = 'dura_orthoDBv11_4447_3193_33090.og.annotations'\n",
    "df = pd.read_csv(anno_file, sep='\\t', header=0)\n",
    "tpm_file = 'counts_transcript_TPM_concise.txt'\n",
    "tpm = pd.read_csv(tpm_file, sep=',', header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df[['query','Description']]\n",
    "df_2 = pd.merge(df_1, tpm, on='query',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Dura og to EG5.1 og."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Function to parse the 'Attributes' column in gff file and convert it into a dictionary of key-value pairs\n",
    "def parse_attributes(attribute_str):\n",
    "    attribute_pairs = attribute_str.split(';')\n",
    "    attribute_dict = {}\n",
    "    for pair in attribute_pairs:\n",
    "        if len(pair.split('=')) == 2:\n",
    "            key, value = pair.split('=')\n",
    "        else:\n",
    "            key = pair.split('=')[0]\n",
    "            value = ''\n",
    "        attribute_dict[key] = value\n",
    "    return attribute_dict\n",
    "\n",
    "def filtered_rows2bedfile(filtered_rows, bedfile):\n",
    "    # takes a list of gene ids and write their regions into a bed file\n",
    "    df = filtered_rows[['Seqid', 'Start','End','ID']]\n",
    "    df_no_duplicates = df.drop_duplicates()\n",
    "    df_sorted = df_no_duplicates.sort_values(by=['Seqid', 'Start'])\n",
    "    # add pound(#) to the first column name - convention of bed file\n",
    "    df_sorted.columns = ['#' + df_sorted.columns[0] if i == 0 else col for i, col in enumerate(df_sorted.columns)]\n",
    "    df_sorted.to_csv(bedfile, sep='\\t', index=False)\n",
    "    return df_sorted\n",
    "\n",
    "# read in the Dura annotation\n",
    "dura_df = pd.read_csv('/home/huan/build/orthologer/dura_orthoDBv11_4447_3193_33090.og.annotations', sep='\\t')\n",
    "# read in the EG5.1 annotation\n",
    "EG51_df = pd.read_csv('/home/huan/build/orthologer/EG5.1_orthoDBv11_4447_3193_33090.og.annotations', sep='\\t')\n",
    "## read the gff file\n",
    "gff_file = '/home/huan/Documents/palm/EG5.1_Genes.V3.gff3'\n",
    "gff_column_names = ['Seqid', 'Source', 'Type', 'Start', 'End', 'Score', 'Strand', 'Phase', 'Attributes']\n",
    "df = pd.read_csv(gff_file, sep='\\t',comment='#', names=gff_column_names)\n",
    "df1 = df['Attributes'].apply(parse_attributes).apply(pd.Series)\n",
    "df_gff = pd.concat([df.drop('Attributes', axis=1), df1], axis=1)\n",
    "\n",
    "trait_list = ['drought','height','fa','fattyacid','fruit','SAUR']    \n",
    "\n",
    "for trait in trait_list:\n",
    "    # read in the catalog file\n",
    "    cat_file = '/home/huan/build/orthologer/dura_{}_catalogue.txt'.format(trait)\n",
    "    cat_df = pd.read_csv(cat_file, sep='\\t', header=None, names=['query','gene'])\n",
    "    # cat_dura_df = dura_df[dura_df['query'].isin(cat_df['query'])]\n",
    "    cat_dura_df = pd.merge(cat_df, dura_df, on='query',how='inner')\n",
    "    cat_dura_df.rename(columns={'query': 'dura_gene'}, inplace=True)\n",
    "    # sorted_df = cat_dura_df.sort_values(by='ODB_OG')\n",
    "    cat_EG51_df = EG51_df[EG51_df['ODB_OG'].isin(cat_dura_df['ODB_OG'])]\n",
    "    shared_columns = set(EG51_df.columns).intersection(set(cat_dura_df.columns))\n",
    "    columns_to_keep = ['ODB_OG'] + [col for col in cat_dura_df.columns if col not in shared_columns]\n",
    "    cat_dura_df_reduced = cat_dura_df[columns_to_keep]\n",
    "    \n",
    "    # Perform the merge\n",
    "    cat_EG51_df = pd.merge(cat_dura_df_reduced, EG51_df, on='ODB_OG', how='inner')\n",
    "    selected_df = cat_EG51_df[['ODB_OG', 'gene', 'dura_gene', 'query']]\n",
    "    selected_df.rename(columns={'query': 'EG5.1_gene', 'gene': 'gene_name'}, inplace=True)\n",
    "    \n",
    "    selected_df.to_csv('~/build/orthologer/EG5.1_{}_catalogue.txt'.format(trait), sep='\\t', index=False)\\\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## read in the orthoDB annotation file\n",
    "    selected_rows = EG51_df[EG51_df['ODB_OG'].isin(selected_df['ODB_OG'])]\n",
    "    selected_rows = selected_rows.rename(columns={'#query': 'query'})\n",
    "    \n",
    "    \n",
    "    # prepare bed file gene region\n",
    "    filtered_rows = df_gff[df_gff['ID'].isin(selected_rows['query'])]\n",
    "    bedfile = '~/build/orthologer/EG5.1_{}_exon.bed'.format(trait)\n",
    "    filtered_rows2bedfile(filtered_rows, bedfile) \n",
    "\n",
    "# prepare bed file for exons only: 9 should be enough? No it is not. can easily go beyond that.\n",
    "# pattern = '|'.join([f\"{item}:[1-9]$\" for item in selected_rows['query']])\n",
    "# pattern = r'^(?:' + '|'.join(selected_rows['query']) + r'):\\d+$'\n",
    "\n",
    "# Filtering rows\n",
    "# filtered_rows = df_gff[df_gff['ID'].str.contains(pattern, regex=True)]\n",
    "# bedfile = gene_name + '_at' + level + '_dura.exon.bed'\n",
    "# filtered_rows2bedfile(filtered_rows, bedfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Synteny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['pic_chr','pic_len','p_start','p_end','orientation','dur_chr','dur_len','d_start','d_end','A','B','C','D','E','AS','nn','tp','cm','s1','s2','df','zd','rl','cg']\n",
    "syn_chr1 = pd.read_csv('/home/huan/Documents/palm/pic_chr1_dura.paf', sep='\\t', names=colnames)\n",
    "df_sorted = syn_chr1.sort_values(by='p_start')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# misssed jobs in freebayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "i = 0\n",
    "dic = {}\n",
    "with open('/home/huan/Downloads/EG5.1_batch2_190_chr1.pos') as fh:\n",
    "    for line in fh:\n",
    "        j = int(line.rstrip())\n",
    "        dis = j-i\n",
    "        dic[i] = dis\n",
    "        i = j\n",
    "i = 0\n",
    "sorted_keys = sorted(dic.keys())\n",
    "\n",
    "with open('/home/huan/Downloads/EG5.1_batch2_262_chr1.pos') as fh:\n",
    "    for line in fh:\n",
    "        j = int(line.rstrip())\n",
    "        if j - i > 10000:\n",
    "            if i in dic:\n",
    "                if dic[i] < 10000:\n",
    "                    print(i,j)\n",
    "            else:\n",
    "                # Find the left and right indices\n",
    "                left_index = bisect.bisect_left(sorted_keys, i)\n",
    "                right_index = bisect.bisect_right(sorted_keys, j)\n",
    "                # Calculate the count\n",
    "                count = right_index - left_index\n",
    "                if count > 1:\n",
    "                    print(i,j)\n",
    "                    break\n",
    "        i = j\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From gff to bed (using ID instead of transciptID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse the attribute column\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def parse_attribute(attribute):\n",
    "    pairs = attribute.split(';')\n",
    "    attr_dict = {}\n",
    "    for pair in pairs:\n",
    "        # Check if the element contains more attributes separated by ','\n",
    "        if pair.count('=') > 1:\n",
    "            if ',' in pair:\n",
    "                sub_pairs = pair.split(',')\n",
    "                for sub_pair in sub_pairs:\n",
    "                    if '=' in sub_pair:\n",
    "                        print(sub_pair)\n",
    "                        key, value = sub_pair.split('=')\n",
    "                        attr_dict[key] = value\n",
    "        else:\n",
    "            if '=' in pair:\n",
    "                key, value = pair.split('=')\n",
    "                attr_dict[key] = value\n",
    "    return attr_dict\n",
    "\n",
    "def gff2bed(gff_file, type = None, bed_level = 'ID'):\n",
    "    gff = pd.read_csv(gff_file, sep='\\t', header=None, comment=\"#\", names=['chr','source','type','start','end','score','strand','phase','attributes'])\n",
    "    if type:\n",
    "        gff = gff[gff['type']==type]\n",
    "    # Apply the function to each row in the 'attribute' column\n",
    "    parsed_attributes = gff['attributes'].apply(parse_attribute).apply(pd.Series)    \n",
    "    # Join the parsed attributes back to the original DataFrame\n",
    "    gff = gff.join(parsed_attributes)\n",
    "    # note that gff is 1-based and bed is 0-based, so start needs to be adjusted\n",
    "    gff['start'] = gff['start'].astype(int) - 1\n",
    "    bed = gff[['chr','start','end',bed_level]].copy()\n",
    "    bed.rename(columns={'chr': '#chr'}, inplace=True)\n",
    "    root, ext = os.path.splitext(gff_file)\n",
    "    bed.to_csv(root+'.bed', sep = '\\t', index=False)\n",
    "def idlist2seqfile(ids, seqfile, outfile):\n",
    "    from Bio import SeqIO\n",
    "    ids = set(ids)\n",
    "    with open(outfile, 'w') as out_fh:\n",
    "        for record in SeqIO.parse(seqfile, 'fasta'):\n",
    "            if record.id in ids:\n",
    "                SeqIO.write(record, out_fh, 'fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gff2bed('/home/huan/Documents/Palm/EG5.1_p5.00_sc00060_p0019.gff3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gff2bed('/home/huan/Documents/Palm/dura_Egu023247.gff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get rRNA genes from EG5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gff = pd.read_csv('/home/huan/Documents/Palm/genomic.gff', sep='\\t', header=None, comment=\"#\", names=['chr','source','type','start','end','score','strand','phase','attributes'])\n",
    "gff_rRNA = gff[gff['type']=='rRNA']\n",
    "\n",
    "# Apply the function to each row in the 'attribute' column\n",
    "parsed_attributes = gff_rRNA['attributes'].apply(parse_attribute).apply(pd.Series)\n",
    "\n",
    "# Join the parsed attributes back to the original DataFrame\n",
    "gff_rRNA = gff_rRNA.join(parsed_attributes)\n",
    "gff_rRNA['id'] = gff_rRNA['ID'].strsplit('-')[1]\n",
    "                \n",
    "seqfile = '/home/huan/Documents/Palm/dura_proteins.fasta'\n",
    "outfile = '/home/huan/Documents/Palm/dura_30unique_proteins.fasta'\n",
    "ids = idfile2seqfile(idfile,seqfile, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gff_rRNA['id'] = gff_rRNA['ID'].str.split('-').str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gff_rRNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
